{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#Next we reshape it so that it is suitable for training a CNN. In Keras, the layers used for two-dimensional convolutions expect\n",
    "#pixel values with the dimensions [pixels][width][height].\n",
    "# In the case of RGB, the first dimension pixels would be 3 for the red, green and blue components. But in Mnist, where the pixel\n",
    "#values are gray scale, the pixel dimension is set to 1.\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "#upto here, we proprocess the data and steps are same as that of MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now create our simple neural network model\n",
    "#Convolutional neural networks are more complex than standard multi-layer perceptrons, so we will start by using a simple \n",
    "#structure to begin with that uses all of the elements for state of the art results i.e excellent results.\n",
    "#Below summarizes the network architecture.\n",
    "#The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps each of size 5×5 and a \n",
    "#relu activation function. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool \n",
    "#size of 2×2.The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of \n",
    "#neurons in the layer in order to reduce overfitting.Next is a layer that converts the 2D matrix data to a vector called Flatten.\n",
    "#It allows the output to be processed by standard fully connected layers.Next a fully connected layer with 128 neurons and \n",
    "#relu activation function.Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output\n",
    "#probability-like predictions for each class.\n",
    "#As before, the model is trained using logarithmic loss using categorical_crossentropy and the ADAM gradient descent algorithm.\n",
    "\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 208s - loss: 0.2328 - acc: 0.9342 - val_loss: 0.0811 - val_acc: 0.9742\n",
      "Epoch 2/10\n",
      " - 202s - loss: 0.0739 - acc: 0.9781 - val_loss: 0.0464 - val_acc: 0.9845\n",
      "Epoch 3/10\n",
      " - 204s - loss: 0.0534 - acc: 0.9840 - val_loss: 0.0431 - val_acc: 0.9862\n",
      "Epoch 4/10\n",
      " - 199s - loss: 0.0405 - acc: 0.9878 - val_loss: 0.0404 - val_acc: 0.9873\n",
      "Epoch 5/10\n",
      " - 200s - loss: 0.0336 - acc: 0.9894 - val_loss: 0.0353 - val_acc: 0.9878\n",
      "Epoch 6/10\n",
      " - 206s - loss: 0.0276 - acc: 0.9916 - val_loss: 0.0313 - val_acc: 0.9896\n",
      "Epoch 7/10\n",
      " - 221s - loss: 0.0233 - acc: 0.9928 - val_loss: 0.0359 - val_acc: 0.9883\n",
      "Epoch 8/10\n",
      " - 200s - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0331 - val_acc: 0.9886\n",
      "Epoch 9/10\n",
      " - 220s - loss: 0.0168 - acc: 0.9944 - val_loss: 0.0300 - val_acc: 0.9902\n",
      "Epoch 10/10\n",
      " - 257s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0307 - val_acc: 0.9905\n",
      "CNN Error: 0.95%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model has 5 hidden layers and 1 output layers, so it is a 6-layered neural network. we found the error to be 0.95%, \n",
    "#which is lesser than MLP and  we try to minimize it further. accuracy obtained=99.05%(greater than MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
