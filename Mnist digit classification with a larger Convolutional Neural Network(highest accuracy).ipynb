{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger CNN for the MNIST Dataset\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "#upto here preprocessing is complete. it is exactly same as simple CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the larger model\n",
    "#This time we define a large CNN architecture with additional convolutional, max pooling layers and fully connected layers. \n",
    "#The network topology can be summarized as follows.\n",
    "\n",
    "#Convolutional layer with 30 feature maps of size 5×5.\n",
    "#Pooling layer taking the max over 2*2 patches.\n",
    "#Convolutional layer with 15 feature maps of size 3×3.\n",
    "#Pooling layer taking the max over 2*2 patches.\n",
    "#Dropout layer with a probability of 20%.\n",
    "#Flatten layer.\n",
    "#Fully connected layer with 128 neurons and relu activation.\n",
    "#Fully connected layer with 50 neurons and relu activation.\n",
    "#Output layer.\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 198s 3ms/step - loss: 0.6666 - acc: 0.7949 - val_loss: 0.1324 - val_acc: 0.9626\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.1376 - acc: 0.9583 - val_loss: 0.0770 - val_acc: 0.9765\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.0964 - acc: 0.9706 - val_loss: 0.0576 - val_acc: 0.9825\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 195s 3ms/step - loss: 0.0731 - acc: 0.9783 - val_loss: 0.0428 - val_acc: 0.9864\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.0630 - acc: 0.9804 - val_loss: 0.0399 - val_acc: 0.9878\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0545 - acc: 0.9833 - val_loss: 0.0350 - val_acc: 0.9883\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.0487 - acc: 0.9846 - val_loss: 0.0329 - val_acc: 0.9893\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 198s 3ms/step - loss: 0.0450 - acc: 0.9857 - val_loss: 0.0275 - val_acc: 0.9912\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.0399 - acc: 0.9876 - val_loss: 0.0273 - val_acc: 0.9900\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 198s 3ms/step - loss: 0.0371 - acc: 0.9885 - val_loss: 0.0271 - val_acc: 0.9908\n",
      "Large CNN Error: 0.92%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=500)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model has 8 hidden layers and 1 output layer. so it is a 9-layered neural network. CNN error=0.92%, which is the least of \n",
    "#the 3 models and accuracy=99.12% which is the highest among the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
